"use strict";(globalThis.webpackChunkgreenwave_doc=globalThis.webpackChunkgreenwave_doc||[]).push([[358],{1817:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/Event-Driven-d671d648b1c6121164f6c34c54fecc27.png"},2378:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"guides/greenwave-ai/intro","title":"Introduction to GreenWave AI","description":"\x3c!--","source":"@site/docs/guides/greenwave-ai/intro.md","sourceDirName":"guides/greenwave-ai","slug":"/guides/greenwave-ai/intro","permalink":"/GreenWave/vi/docs/guides/greenwave-ai/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/sonmessia/GreenWave.git/docs/guides/greenwave-ai/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction to GreenWave AI"},"sidebar":"tutorialSidebar","previous":{"title":"GreenWave AI","permalink":"/GreenWave/vi/docs/category/greenwave-ai"},"next":{"title":"System Running Guide","permalink":"/GreenWave/vi/docs/guides/greenwave-ai/run"}}');var r=i(4848),s=i(8453);const l={sidebar_position:1,title:"Introduction to GreenWave AI"},o="Introduction to GreenWave AI",a={},c=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"Artificial Intelligence &amp; Algorithms",id:"artificial-intelligence--algorithms",level:2},{value:"Algorithm: Deep Q-Network (DQN)",id:"algorithm-deep-q-network-dqn",level:3},{value:"The &quot;Dataset&quot; (Experience Replay)",id:"the-dataset-experience-replay",level:3},{value:"State Space (Input)",id:"state-space-input",level:3},{value:"Action Space (Output)",id:"action-space-output",level:3},{value:"Goal &amp; Reward Function",id:"goal--reward-function",level:3},{value:"Performance &amp; Effectiveness",id:"performance--effectiveness",level:3}];function d(e){const n={blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"introduction-to-greenwave-ai",children:"Introduction to GreenWave AI"})}),"\n",(0,r.jsx)(n.p,{children:"GreenWave AI is a smart traffic light control system utilizing Artificial Intelligence (AI) and Reinforcement Learning, completely integrated with the FIWARE platform and SUMO traffic simulation."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Unlike traditional traffic light systems that use fixed-time counters, GreenWave AI analyzes traffic flow in real-time to make the most optimal control decisions."}),"\n",(0,r.jsxs)(n.p,{children:["The system operates based on the principle: ",(0,r.jsx)(n.strong,{children:'"Green lights are only for directions that truly need them"'}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Independent Control"}),": Each traffic light pillar is analyzed and controlled independently by AI, not rigidly dependent on other lights."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time"}),": Decisions are made every 2 seconds based on current data from sensors."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Conflict-Free"}),": The algorithm ensures safety, ensuring that signal conflicts never occur."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Objective"}),": Simultaneously optimizes multiple metrics: queue length, waiting time, and vehicle density."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,r.jsx)(n.p,{children:"GreenWave AI operates according to the following closed-loop pipeline:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Ai-Architecture.png",src:i(1817).A+"",width:"1553",height:"1847"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Collection"}),": Sensors (Induction loops) in SUMO collect information about speed and vehicle count."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context Broker"}),": Data is normalized into TrafficFlowObserved and sent to Orion-LD."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AI Processing"}),": The AI Agent receives notifications from Orion, analyzes the state, and makes a phase decision."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Execution"}),": Control commands are sent back via the IoT Agent to change the light status in the simulation."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"artificial-intelligence--algorithms",children:"Artificial Intelligence & Algorithms"}),"\n",(0,r.jsxs)(n.p,{children:["GreenWave employs\xa0",(0,r.jsx)(n.strong,{children:"Reinforcement Learning (RL)"}),"\xa0to solve the Traffic Signal Control (TSC) problem. Unlike traditional timer-based systems or static sensors, this AI learns optimal strategies by interacting with the simulation."]}),"\n",(0,r.jsx)(n.h3,{id:"algorithm-deep-q-network-dqn",children:"Algorithm: Deep Q-Network (DQN)"}),"\n",(0,r.jsxs)(n.p,{children:["We use a\xa0",(0,r.jsx)(n.strong,{children:"Double DQN"}),"\xa0(Deep Q-Network) approach to stabilize learning."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Architecture"}),": A multi-layer perceptron (Neural Network) built with\xa0",(0,r.jsx)(n.strong,{children:"TensorFlow/Keras"}),".","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input Layer"}),": 4 Neurons (State State)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hidden Layer 1"}),": 128 Neurons (ReLU activation, Dropout 0.2)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hidden Layer 2"}),": 128 Neurons (ReLU activation, Dropout 0.2)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hidden Layer 3"}),": 64 Neurons (ReLU activation)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output Layer"}),": 2 Neurons (Linear activation) representing Q-values for each action."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimizer"}),": Adam (Learning Rate = 0.0005)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loss Function"}),": Mean Squared Error (MSE)."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"the-dataset-experience-replay",children:'The "Dataset" (Experience Replay)'}),"\n",(0,r.jsx)(n.p,{children:'In Reinforcement Learning, there is no static "dataset" (like CSVs or Images). The AI creates its own dataset to learn from by interacting with the environment:'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Source"}),": Analysis of real-time simulation frames (SUMO)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Experience Replay Buffer"}),": Stores the last\xa0",(0,r.jsx)(n.strong,{children:"10,000"}),"\xa0interaction steps.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Format:\xa0",(0,r.jsx)(n.code,{children:"(State, Action, Reward, Next_State)"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Training"}),': The model samples a random "batch" (size 64) from this buffer to train itself, ensuring it learns from both recent and past experiences (preventing forgetting).']}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"state-space-input",children:"State Space (Input)"}),"\n",(0,r.jsxs)(n.p,{children:['The AI "sees" the intersection through a vector of\xa0',(0,r.jsx)(n.strong,{children:"4 values"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Queue Length"}),"\xa0(Detector 1): Number of cars waiting at the North/South arm."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Queue Length"}),"\xa0(Detector 2): Number of cars waiting at the East/West arm."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Current Phase"}),": Which light is currently green? (Index)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PM2.5 Emission"}),": Total air pollution calculated from vehicle emissions in the area."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"action-space-output",children:"Action Space (Output)"}),"\n",(0,r.jsxs)(n.p,{children:["The AI can make\xa0",(0,r.jsx)(n.strong,{children:"2 Decisions"}),"\xa0at every step:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action 0 (HOLD)"}),": Keep the current light Green."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action 1 (SWITCH)"}),": Switch the light to Red (and the other to Green).","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Constraint"}),": The system enforces a\xa0",(0,r.jsx)(n.strong,{children:"Minimum Green Time"}),"\xa0(e.g., 10 seconds) to prevent chaotic, rapid switching."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"goal--reward-function",children:"Goal & Reward Function"}),"\n",(0,r.jsx)(n.p,{children:'The AI\'s goal is to maximize the "Reward". Our reward function is designed to balance traffic flow and environmental impact:'}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"Reward = (0.6 \xd7 -TotalQueue) + (0.4 \xd7 -TotalPM2.5)"}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Logic"}),": The reward is always negative (a penalty). The AI tries to get this number as close to zero as possible."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Traffic Priority (60%)"}),": Minimizing queues is the main goal."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Eco Priority (40%)"}),": If two actions reduce queues equally, the AI picks the one that generates less pollution (e.g., keeping heavy trucks moving)."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"performance--effectiveness",children:"Performance & Effectiveness"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Adaptive"}),": Unlike fixed-time lights, this system adapts to rush hour vs. midnight traffic automatically."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-objective"}),": It solves not just for speed, but for\xa0",(0,r.jsx)(n.strong,{children:"Sustainability (Green Wave)"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluation"}),": During training, we observed a reduction in average waiting times compared to static timing, specifically in high-load scenarios (saturated intersections)."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var t=i(6540);const r={},s=t.createContext(r);function l(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);